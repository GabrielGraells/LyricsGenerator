{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drpYXXaC_noX"
   },
   "source": [
    "# Deep Learning final project - Lyrics Generation\n",
    "\n",
    "---\n",
    "\n",
    "    Gabriel Graells - 205638\n",
    "\n",
    "---\n",
    "For this project, we implement a RNN capable of generating new song lyrics character by character after being trained with hundreds of reggeaton songs of the best singers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-B3Y6rvpNDm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFh3Jg_graE_"
   },
   "source": [
    "# Creating our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEGjZuiXreZ2"
   },
   "source": [
    "## Load the Data\n",
    "\n",
    "Load the lyrics text file and convert it into integers for our network to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24134,
     "status": "ok",
     "timestamp": 1591825222413,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "g01ZIWGosL3G",
    "outputId": "48ee1fcc-eb67-4e14-f994-8ed0ffe5d19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "#mount Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yd2t9lhnK4Tq"
   },
   "source": [
    "# Data Collection\n",
    "In order to generated the needed data set for training we have used [this](https://github.com/johnwmillr/LyricsGenius) library that has been build on top of the API of the music content website [Genius](https://genius.com/).\n",
    "The code searches for the top 50 songs, based on popularity, of the artist in the list, then downloads one JSON file per artist containing the songs and further information. Finally it transfers all songs from the JSONs file to the output file \"*lyrics.txt*\".\n",
    "\n",
    "**If you want to execute this part of the code is better to do it locally in a *.py* Python script.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjDNMZBaM0BV"
   },
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "artist_list = ['Bad Bunny', 'Daddy Yanky','Ozuna', 'Don Omar', 'Anuel AA','Maluma','Juan Magan','Nicky Jam', 'Karol G','Mozart La Para', 'Cosculluela', 'Tito \"El Bambino\"','Calle Trece', 'Arcangel','Tego Calderon','Plan B']\n",
    "\n",
    "#Init API client\n",
    "genius = lyricsgenius.Genius(\"XAjzWCsgJw9mHU48O7RRZ6-nzXykKtYH9_7zAFnPbl_PHrVQZQBM7InGU05sji9o\")\n",
    "genius.verbose = False \n",
    "genius.remove_section_headers = True\n",
    "genius.excluded_terms = ['Live', '(Live)']\n",
    "\n",
    "#Iterate artist_list and download JSON file with lyrics\n",
    "for art in artist_list:\n",
    "    print(art)\n",
    "    artist = genius.search_artist(art, max_songs = 50, sort='popularity')\n",
    "    \n",
    "    if artist == None:\n",
    "        print(art,'Does not exist!')\n",
    "    else:\n",
    "        print('saving json...')\n",
    "        artist.save_lyrics()\n",
    "\n",
    "#Get names json files in current directory\n",
    "json_files = [pos_json for pos_json in os.listdir() if pos_json.endswith('.json')]\n",
    "\n",
    "for files in json_files:\n",
    "    with open(files) as json_file, open('lyrics.txt', 'a') as text_file:\n",
    "        data = json.load(json_file)\n",
    "        for s in data['songs']:  \n",
    "            text_file.write(s['lyrics'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6A1DP0WtN271"
   },
   "source": [
    "# Data Cleaning\n",
    "After the \"lyrics.txt\" file is generated we need to review the downloaded data and define a criteria to clean our it with the goal of improving the network performance.\n",
    "The model for our lyrics generator learns on a char level, meaning it uses a dictionary of characters and learns which character is the more likely to follow a given \n",
    "sequence. Thus reducing the amount of characters will \n",
    "reduce the decision range and the complexity of the problem.\n",
    "\n",
    "Initially our data contained a total of 131 characters, we printed them to inspect them and we found Korean and Germanm letters. For this cases we took the extreme approach of deleting the songs containing those characters. \n",
    "\n",
    "Still, there where remaining a series of simbols in the character dictionary which we deleted them with the following code snipped:\n",
    "```\n",
    "f1 = open('lyrics.txt', 'r')\n",
    "f2 = open('lyrics.txt', 'w')\n",
    "for line in f1:\n",
    "    f2.write(line.replace(, ))\n",
    "f1.close()\n",
    "f2.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PA9cSj5Sami"
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AjX3OaIDtXi9"
   },
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('/content/drive/My Drive/DeepLearning_2020/FP/lyrics.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1591790010437,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "jVxtITJmpND7",
    "outputId": "df3b6b24-a175-463b-9c9b-29a5e73b1a74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"---\\nY esta noche está pa' bailar, beber, jode'\\nHasta que no pueda más (Yeh-yeh-yeh-yeh)\\nY esta noche\""
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuSbSxppSkrN"
   },
   "source": [
    "# Data Encoding\n",
    "A neural neural model needs numerical inputs for a better performance so we tranform each char into an interger by simply enumerating all the unique characters in out lyrics file. We generate two dictionaries, ***int2char*** maps an integer value to the associated character and ***char2int*** does the inverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xRBEK92pNEC"
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars)) #maps integers to characters\n",
    "char2int = {ch: ii for ii, ch in int2char.items()} #maps characters to unique integers\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1327,
     "status": "ok",
     "timestamp": 1591825239874,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "AkKDujngpNEL",
    "outputId": "baab970b-0d0d-48c2-8635-5338aeab2152"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68, 68, 68, 17, 76, 84, 37, 47, 40, 63, 84, 93, 18, 20, 80, 37, 84,\n",
       "       37, 47, 40, 30, 84, 70, 63, 87, 84, 67, 63, 96, 69, 63, 53, 56, 84,\n",
       "       67, 37, 67, 37, 53, 56, 84, 44, 18, 42, 37, 87, 17, 12, 63, 47, 40,\n",
       "       63, 84, 41, 25, 37, 84, 93, 18, 84, 70, 25, 37, 42, 63, 84, 15, 30,\n",
       "       47, 84, 74, 76, 37, 80, 68, 50, 37, 80, 68, 50, 37, 80, 68, 50, 37,\n",
       "       80, 64, 17, 76, 84, 37, 47, 40, 63, 84, 93, 18, 20, 80, 37])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jt1i413PV8QQ"
   },
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "As stated before is is mandatory to transform the character to a numerical representation. \n",
    "\n",
    "One hot encoding is a commom practice for categorical data and for text generation.\n",
    "\n",
    "One Hot encoding consists on representing each character as a binary vector of N dimensions, where N is the number of categories (characters) in the data, it would have a 1 in the position associated to that character and 0 in the rest of positions.\n",
    "\n",
    "If we have a dictionary with three characters [a, t, c] and we want to encode the word \"cat\" it would be represent it as follows:\n",
    "\n",
    "[[0 0 1]\n",
    "[1 0 0]\n",
    "[0 1 0]].\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Why One Hot Encoding?**\n",
    "\n",
    "Even though we could use Interger Encoding One Hot Encoding preserves the independence between categories outputing much better results.\n",
    "\n",
    "Integer encoding introduces a natural continuity between categories, assume we map value 2 with letter 'a' and 3 to letter 't'. There is a natural numerical continuity between 2 and 3, so if the actual output **y** should be 3 and the model guesses 2.49 the answer is close enought to our target. If we translate this logic to characters we can appreciate that there is no actual continuity between letter 'a' to letter 't'.\n",
    "\n",
    "One Hot encoding brakes this continuity by generating N buckets (one for each category), the model gives \"points\" to a bucket if the associated character is likely to follow the senquence. The char with more \"points\" in its bucket is the one selected to follow the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "001we_cipNET"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the correct index with a one for each char\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1591825250611,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "Kimc1gaNpNEZ",
    "outputId": "99d22640-f446-4e96-de3d-73519500e1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-XmRfX1pNEg"
   },
   "outputs": [],
   "source": [
    "#get_batches function returns batches of size batch_size x seq_length from array\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # Get the number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "faOhnpR1pNEo"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1591825264744,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "RpJ-UE_JpNEu",
    "outputId": "a955a665-4db1-4705-a13e-8d4bab73255d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[68 68 68 17 76 84 37 47 40 63]\n",
      " [83 63 42 63 84  9 37 62 84 41]\n",
      " [37 33 25 53 18 84 37 93 84 15]\n",
      " [84 37 69 84 20 63 47 37 53 91]\n",
      " [47 37 84 41 25 37 42 63 84 47]\n",
      " [41 25 37 84 15 37 84 80 96 20]\n",
      " [63 84 70 96 69 63 17 54 63 84]\n",
      " [63 69 17 54 53 53 53 63 63 17]]\n",
      "\n",
      "y\n",
      " [[68 68 17 76 84 37 47 40 63 84]\n",
      " [63 42 63 84  9 37 62 84 41 25]\n",
      " [33 25 53 18 84 37 93 84 15 96]\n",
      " [37 69 84 20 63 47 37 53 91 18]\n",
      " [37 84 41 25 37 42 63 84 47 18]\n",
      " [25 37 84 15 37 84 80 96 20 96]\n",
      " [84 70 96 69 63 17 54 63 84 41]\n",
      " [69 17 54 53 53 53 63 63 17 12]]\n"
     ]
    }
   ],
   "source": [
    "#Print the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1591825273036,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "q8KqBg2XpNE4",
    "outputId": "8fd0ace4-51ea-4ce5-8aff-261522aff2a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "  print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9ZnkvYKpNFA"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        #LSTM layer\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        #fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                \n",
    "        #Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "      \n",
    "        #pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs \n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fQ1wnVVpNFI"
   },
   "outputs": [],
   "source": [
    "#function to train the network\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backpropagate through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backpropagation\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # we use Clipping_gradiend_norm method to prevent the exploding gradient problem\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-n2n1OnpNFP"
   },
   "outputs": [],
   "source": [
    "#set model hyperparameters\n",
    "n_hidden= 1024\n",
    "n_layers= 3\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10323011,
     "status": "ok",
     "timestamp": 1591810473916,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "EFNKXP1XpNFW",
    "outputId": "46ee28b6-a98d-4761-9595-ba543a1f54b1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 100... Loss: 3.2262... Val Loss: 3.2244\n",
      "Epoch: 2/100... Step: 200... Loss: 3.2258... Val Loss: 3.2213\n",
      "Epoch: 3/100... Step: 300... Loss: 3.2383... Val Loss: 3.2169\n",
      "Epoch: 4/100... Step: 400... Loss: 3.2089... Val Loss: 3.1973\n",
      "Epoch: 5/100... Step: 500... Loss: 3.1048... Val Loss: 3.1065\n",
      "Epoch: 6/100... Step: 600... Loss: 2.7763... Val Loss: 2.7712\n",
      "Epoch: 7/100... Step: 700... Loss: 2.7067... Val Loss: 2.6009\n",
      "Epoch: 8/100... Step: 800... Loss: 2.4637... Val Loss: 2.4378\n",
      "Epoch: 9/100... Step: 900... Loss: 2.4098... Val Loss: 2.3649\n",
      "Epoch: 10/100... Step: 1000... Loss: 2.3327... Val Loss: 2.3228\n",
      "Epoch: 11/100... Step: 1100... Loss: 2.2944... Val Loss: 2.2608\n",
      "Epoch: 12/100... Step: 1200... Loss: 2.2552... Val Loss: 2.2170\n",
      "Epoch: 13/100... Step: 1300... Loss: 2.2124... Val Loss: 2.1773\n",
      "Epoch: 14/100... Step: 1400... Loss: 2.2032... Val Loss: 2.1396\n",
      "Epoch: 15/100... Step: 1500... Loss: 2.1574... Val Loss: 2.1052\n",
      "Epoch: 16/100... Step: 1600... Loss: 2.1262... Val Loss: 2.0705\n",
      "Epoch: 17/100... Step: 1700... Loss: 2.0926... Val Loss: 2.0379\n",
      "Epoch: 18/100... Step: 1800... Loss: 2.0469... Val Loss: 2.0079\n",
      "Epoch: 19/100... Step: 1900... Loss: 2.0363... Val Loss: 1.9818\n",
      "Epoch: 20/100... Step: 2000... Loss: 2.0065... Val Loss: 1.9521\n",
      "Epoch: 21/100... Step: 2100... Loss: 2.0254... Val Loss: 1.9274\n",
      "Epoch: 22/100... Step: 2200... Loss: 1.9253... Val Loss: 1.9017\n",
      "Epoch: 23/100... Step: 2300... Loss: 1.9330... Val Loss: 1.8756\n",
      "Epoch: 24/100... Step: 2400... Loss: 1.8819... Val Loss: 1.8576\n",
      "Epoch: 25/100... Step: 2500... Loss: 1.8803... Val Loss: 1.8370\n",
      "Epoch: 26/100... Step: 2600... Loss: 1.8633... Val Loss: 1.8148\n",
      "Epoch: 27/100... Step: 2700... Loss: 1.7958... Val Loss: 1.7969\n",
      "Epoch: 28/100... Step: 2800... Loss: 1.8549... Val Loss: 1.7749\n",
      "Epoch: 29/100... Step: 2900... Loss: 1.7648... Val Loss: 1.7587\n",
      "Epoch: 30/100... Step: 3000... Loss: 1.7413... Val Loss: 1.7455\n",
      "Epoch: 31/100... Step: 3100... Loss: 1.7522... Val Loss: 1.7315\n",
      "Epoch: 32/100... Step: 3200... Loss: 1.6981... Val Loss: 1.7178\n",
      "Epoch: 33/100... Step: 3300... Loss: 1.7066... Val Loss: 1.7073\n",
      "Epoch: 34/100... Step: 3400... Loss: 1.7299... Val Loss: 1.6925\n",
      "Epoch: 34/100... Step: 3500... Loss: 1.6487... Val Loss: 1.6854\n",
      "Epoch: 35/100... Step: 3600... Loss: 1.7443... Val Loss: 1.6677\n",
      "Epoch: 36/100... Step: 3700... Loss: 1.6866... Val Loss: 1.6598\n",
      "Epoch: 37/100... Step: 3800... Loss: 1.6603... Val Loss: 1.6477\n",
      "Epoch: 38/100... Step: 3900... Loss: 1.6260... Val Loss: 1.6384\n",
      "Epoch: 39/100... Step: 4000... Loss: 1.6359... Val Loss: 1.6403\n",
      "Epoch: 40/100... Step: 4100... Loss: 1.6360... Val Loss: 1.6248\n",
      "Epoch: 41/100... Step: 4200... Loss: 1.5976... Val Loss: 1.6131\n",
      "Epoch: 42/100... Step: 4300... Loss: 1.6250... Val Loss: 1.6064\n",
      "Epoch: 43/100... Step: 4400... Loss: 1.5622... Val Loss: 1.6037\n",
      "Epoch: 44/100... Step: 4500... Loss: 1.5887... Val Loss: 1.5949\n",
      "Epoch: 45/100... Step: 4600... Loss: 1.5363... Val Loss: 1.5881\n",
      "Epoch: 46/100... Step: 4700... Loss: 1.5753... Val Loss: 1.5845\n",
      "Epoch: 47/100... Step: 4800... Loss: 1.5310... Val Loss: 1.5798\n",
      "Epoch: 48/100... Step: 4900... Loss: 1.5391... Val Loss: 1.5734\n",
      "Epoch: 49/100... Step: 5000... Loss: 1.4969... Val Loss: 1.5688\n",
      "Epoch: 50/100... Step: 5100... Loss: 1.5082... Val Loss: 1.5678\n",
      "Epoch: 51/100... Step: 5200... Loss: 1.4908... Val Loss: 1.5597\n",
      "Epoch: 52/100... Step: 5300... Loss: 1.4811... Val Loss: 1.5613\n",
      "Epoch: 53/100... Step: 5400... Loss: 1.4747... Val Loss: 1.5542\n",
      "Epoch: 54/100... Step: 5500... Loss: 1.4834... Val Loss: 1.5518\n",
      "Epoch: 55/100... Step: 5600... Loss: 1.4673... Val Loss: 1.5494\n",
      "Epoch: 56/100... Step: 5700... Loss: 1.4486... Val Loss: 1.5456\n",
      "Epoch: 57/100... Step: 5800... Loss: 1.4165... Val Loss: 1.5482\n",
      "Epoch: 58/100... Step: 5900... Loss: 1.4419... Val Loss: 1.5432\n",
      "Epoch: 59/100... Step: 6000... Loss: 1.4268... Val Loss: 1.5431\n",
      "Epoch: 60/100... Step: 6100... Loss: 1.4133... Val Loss: 1.5416\n",
      "Epoch: 61/100... Step: 6200... Loss: 1.4633... Val Loss: 1.5404\n",
      "Epoch: 62/100... Step: 6300... Loss: 1.3891... Val Loss: 1.5419\n",
      "Epoch: 63/100... Step: 6400... Loss: 1.3792... Val Loss: 1.5363\n",
      "Epoch: 64/100... Step: 6500... Loss: 1.4114... Val Loss: 1.5465\n",
      "Epoch: 65/100... Step: 6600... Loss: 1.3586... Val Loss: 1.5479\n",
      "Epoch: 66/100... Step: 6700... Loss: 1.3401... Val Loss: 1.5358\n",
      "Epoch: 67/100... Step: 6800... Loss: 1.3982... Val Loss: 1.5405\n",
      "Epoch: 67/100... Step: 6900... Loss: 1.3774... Val Loss: 1.5390\n",
      "Epoch: 68/100... Step: 7000... Loss: 1.3211... Val Loss: 1.5313\n",
      "Epoch: 69/100... Step: 7100... Loss: 1.3635... Val Loss: 1.5328\n",
      "Epoch: 70/100... Step: 7200... Loss: 1.3252... Val Loss: 1.5308\n",
      "Epoch: 71/100... Step: 7300... Loss: 1.3637... Val Loss: 1.5323\n",
      "Epoch: 72/100... Step: 7400... Loss: 1.3163... Val Loss: 1.5458\n",
      "Epoch: 73/100... Step: 7500... Loss: 1.3362... Val Loss: 1.5313\n",
      "Epoch: 74/100... Step: 7600... Loss: 1.3011... Val Loss: 1.5373\n",
      "Epoch: 75/100... Step: 7700... Loss: 1.2721... Val Loss: 1.5463\n",
      "Epoch: 76/100... Step: 7800... Loss: 1.2605... Val Loss: 1.5357\n",
      "Epoch: 77/100... Step: 7900... Loss: 1.2399... Val Loss: 1.5432\n",
      "Epoch: 78/100... Step: 8000... Loss: 1.2357... Val Loss: 1.5442\n",
      "Epoch: 79/100... Step: 8100... Loss: 1.2186... Val Loss: 1.5436\n",
      "Epoch: 80/100... Step: 8200... Loss: 1.2331... Val Loss: 1.5451\n",
      "Epoch: 81/100... Step: 8300... Loss: 1.2387... Val Loss: 1.5539\n",
      "Epoch: 82/100... Step: 8400... Loss: 1.2433... Val Loss: 1.5477\n",
      "Epoch: 83/100... Step: 8500... Loss: 1.2125... Val Loss: 1.5574\n",
      "Epoch: 84/100... Step: 8600... Loss: 1.1745... Val Loss: 1.5565\n",
      "Epoch: 85/100... Step: 8700... Loss: 1.2425... Val Loss: 1.5607\n",
      "Epoch: 86/100... Step: 8800... Loss: 1.1732... Val Loss: 1.5648\n",
      "Epoch: 87/100... Step: 8900... Loss: 1.1576... Val Loss: 1.5540\n",
      "Epoch: 88/100... Step: 9000... Loss: 1.1700... Val Loss: 1.5638\n",
      "Epoch: 89/100... Step: 9100... Loss: 1.1269... Val Loss: 1.5725\n",
      "Epoch: 90/100... Step: 9200... Loss: 1.2007... Val Loss: 1.5769\n",
      "Epoch: 91/100... Step: 9300... Loss: 1.1596... Val Loss: 1.5694\n",
      "Epoch: 92/100... Step: 9400... Loss: 1.1782... Val Loss: 1.5719\n",
      "Epoch: 93/100... Step: 9500... Loss: 1.1432... Val Loss: 1.5850\n",
      "Epoch: 94/100... Step: 9600... Loss: 1.1358... Val Loss: 1.5926\n",
      "Epoch: 95/100... Step: 9700... Loss: 1.1819... Val Loss: 1.5878\n",
      "Epoch: 96/100... Step: 9800... Loss: 1.0929... Val Loss: 1.5889\n",
      "Epoch: 97/100... Step: 9900... Loss: 1.1023... Val Loss: 1.5940\n",
      "Epoch: 98/100... Step: 10000... Loss: 1.0735... Val Loss: 1.6169\n",
      "Epoch: 99/100... Step: 10100... Loss: 1.0659... Val Loss: 1.6155\n",
      "Epoch: 100/100... Step: 10200... Loss: 1.0966... Val Loss: 1.6017\n",
      "Epoch: 100/100... Step: 10300... Loss: 1.1189... Val Loss: 1.6294\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 100\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.0001, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cs_YYfNUpNFd"
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAOXx5NEpNFf"
   },
   "outputs": [],
   "source": [
    "model_name = f'rnn_{n_epochs}_epoch.pt'\n",
    "results_path = '/content/drive/My Drive/DeepLearning_2020/FP/Results/'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "torch.save(checkpoint, results_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTigLVKipNGU"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M2P2wBNpNGl"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='Baila', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    #run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2271,
     "status": "ok",
     "timestamp": 1591830716056,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "_npcWvmapNGw",
    "outputId": "75188761-431f-400a-8e2f-c8fcb17d439c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aprende profundo8\"8r8\"r8\"l8ÚÚ\"rl\"8lÚrlÚr\"\"ÚÚÚÚrrrr8rlrÚ8llr8Ú\"Ú\"Ú\"8Ú8l8\"lÚrÚ88\"88l\"l8lr8rrlÚrrÚÚÚrlrllÚrÚÚr\"lll8Ú\"ÚllÚÚÚ8r\"rl8r\"8r88llÚÚrr8Úl88r\"\"rrl8Úr\"l8r\"ÚÚÚlr8Ú\"\"Ú8l\"\"r\"\"lÚ\"Úl8Úr\"8lÚ\"8Ú8Úrrr\"r88lrÚl\"r\"\"\"lÚrl8rrl8rÚl8Ú8lr\"Úr\"8rÚ\"lÚr8rrÚ\"8\"ÚÚ88l8Ú\"ÚÚÚlr8ÚÚrÚlrr\"l\"8ÚÚ8llÚr\"\"rlrÚ8rÚrÚ\"Ú\"Úr8Ú88ÚÚlr\"\"8r\"Ú88\"rlrÚ8ÚÚ\"Ú8lÚl8r88ÚllllÚrÉrl8\"Ú\"Ú8lr\"\"\"\"Ú8lrll88Ú\"8llr\"lÚ8ÚÚ8\"rÚrÚÚr\"rr8\"\"Úl8ÚÚ\"ÚÚÚ\"l\"8rÚ\"rr88\"Úrl8ÚlÚÚ8rÚÚlr\"\"lÚ8l\"l88rll\"l\"Él\"l8rÚrÚ8rr\"\"rrlrl\"r8rÚ\"rl8Úlrr\"l\"\"\"r8ÚÚrrlÚÚ8\"l\"lr8Ú8l\"8rlÚrllÚlÉÉlÚ88\"ÚrÚ88rrrrl\"l8ÚrÚ8rÚl8Ú\"lllÉr888\"rÚlllrr8ÚlÚr8l8rlrlrÚÚÚrr\"lrr88\"Úl8\"ÚÚlrÚ\"r\"lrl8\"rÚr\"8l8\"\"88rÚlrÚrl8\"\"r\"llll8É88r8\"8r\"8\"rÚ8l8\"l8\"8rrlrlÚrÚlrr\"ll\"ÚÉ\"rl8lr8ÚÚ8lrÚr\"l\"l\"l8rÚ8lÚ888\"\"lr\"lÚ88ÚÚr\"l\"\"Ú\"ÚÚÚlrll\"8r8Úr8\"8l\"ÚÚÚrrÚ\"Ú88rl\"8Úr8Ú\"ÚÚr\"\"rllÚÚ8lÚ8\"888ÚÚ8\"\"\"l\"l8lrlrÚÚl8\"\"ÚlÚl8l88Ú8r\"\"\"Úlrlr8\"rlllrrl\"rÚ\"\"l8rrÚ8\"lllr8r\"Ú8rlrlÚÚÚr8\"\"\"8lÚ8ll8lÚ8r8\"8l\"8l\"r8\"rÚlr\"8lllÚrÚ\"llrÚl\"8l\"rr888\"\"8l8\"ÚlrÚÚÚrlÚr8\"8l8\"l\"\"ÚÚr\"\"ÚÚ88r\"\"\"8ÚÚrÚllÚll\"Úlrrr8Úll8Úr\"Úrrr\"rÚÚr\"rÚl\"lÚrÚ8Ú\"lÚrlÚÚÚlllÚÚ\"8Ú\"Ú\"8rÚÚÚlllrllÉlÉÚ\"ÚlllÉÚr\"r8r\"rr\"r8\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Aprende profundo', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10172,
     "status": "ok",
     "timestamp": 1591825375646,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "oqCg5pBXpNHG",
    "outputId": "4e030dad-04c6-49e7-e7d0-2884cb84b67f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have loaded in a model that trained over 100 epochs `rnn_100_epoch.net`\n",
    "with open('/content/drive/My Drive/DeepLearning_2020/FP/Results/rnn_100_epoch.ckpt', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3622,
     "status": "ok",
     "timestamp": 1591830782709,
     "user": {
      "displayName": "GABRIEL GRAELLS",
      "photoUrl": "",
      "userId": "06318594296163771906"
     },
     "user_tz": -120
    },
    "id": "CVhwdynbpNHU",
    "outputId": "49d9df74-e52b-48b5-8192-eaa623afb200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baila con mi corazón\n",
      "Yo sé que tú te vas, volvera a pensar\n",
      "Y ahora te tengo amigos que me desestene\n",
      "Yo sé que tú me preguntas a que no te vender a mi\n",
      "Y si yo me voy pa'l party\n",
      "Y aunque te pido perdón\n",
      "Yo nunca voy a porer el perremo\n",
      "Y ahora soy pero, pero yo no sé\n",
      "\n",
      "No paro de pensar en ti como tú no metes\n",
      "Yandele esa no queda nosotros (Nos damos)\n",
      "Y es que yo quiero estar contigo hoy (Estoy antendo)\n",
      "Yo que tú quieras tener (Ah)\n",
      "Yo te llego tú me desea'\n",
      "Que yo no tengo miedo a que no te quedas no pares no me desesperes\n",
      "Y que te prefieres a mí\n",
      "Yo no sé cómo aquí no me desespero\n",
      "Quiero decirte aquí, ahora es mejor que no\n",
      "Cuando yo te pienso y tú ere' mi corazón\n",
      "Yo te lo meto como tú, tú, tú, tú, tú, tú, tú\n",
      "\n",
      "Y aunque tú tienes la milla y se me hace los tiempos (Yeh-yeh-yeh)\n",
      "No te amo estar no puedo cantar\n",
      "Y aunque tú te vayas (Yah)\n",
      "Que en la pida hace calor, yeah-eh-eh (Yeh-eh-eh)\n",
      "Y es que a mi me toca la vida\n",
      "Yo sé que tú me deseas\n",
      "Yo me puedo contener\n",
      "No me dije que te pido\n",
      "Que no te quiero y más que me haces mejor\n",
      "Que yo te entrego mis presos\n",
      "Y tú eres mi coma\n",
      "Te encanta de mis por de amante\n",
      "Y aunque te hacía amor\n",
      "Dime si voy a surar\n",
      "\n",
      "Yo tengo un pas de tomos al destimos\n",
      "Y aunque te ama en mi cama\n",
      "Yo te quiero a mi mente y me desespero en mi\n",
      "Pero el mismo soy tu pasado\n",
      "Y tú eres la quien me das\n",
      "Si te vas a llevar cuando te ama\n",
      "Te extraño en tus basas\n",
      "Que te perdona y tú no lo agarres\n",
      "Y que no te vas a ser fumando\n",
      "Yo siempre quise ser ti y yo\n",
      "Y al que no te ponga baila yo te pido\n",
      "Tú no vas a ver, que no te quiero más\n",
      "Y tú misma decíss y yo\n",
      "\n",
      "Y si te casa de mi como yo te amo\n",
      "Tú no vas a volver (Nah, nah)\n",
      "Y la noche te tiene' que decirir\n",
      "No sé de ella no, no quiere estar\n",
      "Y si tú no estás yo no se vivir si te acuerdas (Eh-eh)\n",
      "Y aunque me hago sentir tu pelo\n",
      "Y si te vas a ver, yo te voy a llevar (Oh-oh-oh, oh-oh)\n",
      "Y ahora solo te voy a hacer saber\n",
      "Que yo soy el que te llama te dijo que en mi cama te acuerdas\n",
      "Yo te pido por lo que te diga que yo te perdí\n",
      "Y si me dices que tú nade c\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=2, prime=\"Baila\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
